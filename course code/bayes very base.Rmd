---
title: "bayes very base"
author: "Jiahao Tian"
date: "2023-01-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# define a function
# spicify this function inside the '{}'
likelihood = function(n, y, theta){return(theta^y * (1 - theta)^(n - y))}

# creat (define) sequnce of points b/t 0 and 1, then plot likehood value over this sequence
theta = seq(from = 0.01, to = 0.99, by = 0.01)

plot(theta, likelihood(400, 72, theta))
abline(v = 0.18)

# from plot you can see that the max value is about '0.18'

```

```{r}
# define a function

loglike = function(n, y, theta){return(y * log(theta) + (n - y) * log(1 - theta))}
plot(theta, loglike(400, 72, theta)) #is a smooth function hard to see maximun is.

abline(v = 0.18)

plot(theta, loglike(400, 72, theta), type = "l") # 'l' means line plot

```

```{r}
# beta binomial conjugate analysis in working with beta and binomial distribution

# Suppose we are giving two students a multiple choice exam with 40 questions. For each questions has four choices. 
# We don't know how much the students have studied for this exam but we think that they will do better than just
# guessing randomly.

# 1. First, what are our parameters of interest?

# answer: Our parameters of interest are theta 1 being the true probability that the first student will answer a question correctly. And theta 2 being the true probability the second student will answer a question correctly.


# 2. what is our likelihood?

# answer: We're going to use the binomial likelihood, with n equals 40. And then probability is theta, either theta 1 or theta 2. And this is assuming that each question is independent and that the probability a student gets each question right is the same for all questions for that student.

```

```{r}
# 3. What prior should we use?

# The conjugate prior for binomial likelihood is a beta prior.
# The information we have is that we think they will do better than just guessing randomly. So the prior mass should be largely above 0.25. But what a prior mean bigger than a half maybe say, two-thirds. So what I'm going to do is I'm going to make a sequence of possible theta values, that goes from zero to one in increments of 100th.

theta = seq(from = 0, to = 1, by = 0.01)
plot(theta, dbeta(theta, 1, 1), type = "l") # defalut
plot(theta, dbeta(theta, 4, 2), type = "l") # better than guess, maybe prior mean = 2/3 which alpha = 4, beta = 2
abline(v = 0.25)
# now the mass is higer, but some still blow 0.25


plot(theta, dbeta(theta, 8, 4), type = "l")
abline(v = 0.25)
# more better now, distribution is b/t 0.25 and 1.

```

```{r}
# 4. For this prior for theta you can ask what's the prior probability that theta is greater than one quarter which
# would be guessing randomly? Theta is greater than one half or theta's greater than 0.8.

# answer: because ask greater so 1 - CDF

1 - pbeta(0.25, 8, 4)
1 - pbeta(1/2, 8, 4)
1 - pbeta(0.8, 8, 4)

```

```{r}
# 5. suppose the first student takes the test and gets 33 of the 40 questions right calling their parameter theta1, 
# we can ask what's posterior distribution for their parameter? What's the posterior probabilities that would be
# greater than one quarter, one half or 0.8? What's the 95% posterior credible interval for this parameter? 

# answer: Posterior is Beta(8+33,4+40-33) = Beta(41,11)

41/(41+11)  # posterior mean
33/40       # MLE

plot(theta, dbeta(theta,41,11))

# plot posterior first to get the right scale on the y-axis
plot(theta,dbeta(theta,41,11),type="l")
lines(theta,dbeta(theta,8,4),lty=2)
# plot likelihood
lines(theta,dbinom(33,size=40,p=theta),lty=3)
# plot scaled likelihood
lines(theta,44*dbinom(33,size=40,p=theta),lty=3)

# posterior probabilities
1-pbeta(.25,41,11)
1-pbeta(.5,41,11)
1-pbeta(.8,41,11)

# equal-tailed 95% credible interval
qbeta(.025,41,11)
qbeta(.975,41,11)

```


```{r}
# 6. Now thinking about the second student, suppose they get 24 questions right. What's suppose to your distribution
# for the parameter? What's suppose to your probably it's greater than one quarter, one half of 0.8, and what's a 95% credible interval for it?

# answer: Posterior is Beta(8+24,4+40-24) = Beta(32,20)
32/(32+20)  # posterior mean
24/40       # MLE

plot(theta,dbeta(theta,32,20),type="l")
lines(theta,dbeta(theta,8,4),lty=2)
lines(theta,44*dbinom(24,size=40,p=theta),lty=3)

1-pbeta(.25,32,20)
1-pbeta(.5,32,20)
1-pbeta(.8,32,20)

qbeta(.025,32,20)
qbeta(.975,32,20)

```

```{r}
# 7 . Finally, what's suppose to your probability that theta1 greater than theta2? That the first student has a better chance of getting a question right than the second student.

# answer: Estimate by simulation: draw 1,000 samples from each and see how often 
#    we observe theta1>theta2

theta1=rbeta(1000,41,11)
theta2=rbeta(1000,32,20)
mean(theta1>theta2)
```

################################################################################

## Regression Part

```{R, eval=F, echo=T}
## Regression Part

# http://www.randomservices.org/random/data/Challenger2.txt
# 23 previous space shuttle launches before the Challenger disaster
# T is the temperature in Fahrenheit, I is the O-ring damage index

oring = read.table("http://www.math.uah.edu/stat/data/Challenger2.txt", header=T)

attach(oring)
#note: masking T=TRUE

plot(T,I)

oring.lm=lm(I~T)
summary(oring.lm)

# add fitted line to scatterplot
lines(T,fitted(oring.lm))            
# 95% posterior interval for the slope
-0.24337 - 0.06349*qt(.975,21)
-0.24337 + 0.06349*qt(.975,21)
# note that these are the same as the frequentist confidence intervals

# the Challenger launch was at 31 degrees Fahrenheit
# how much o-ring damage would we predict?
# y-hat
18.36508-0.24337*31
coef(oring.lm)
coef(oring.lm)[1] + coef(oring.lm)[2]*31  

# posterior prediction interval (same as frequentist)
predict(oring.lm,data.frame(T=31),interval="predict")  
10.82052-2.102*qt(.975,21)*sqrt(1+1/23+((31-mean(T))^2/22/var(T)))

# posterior probability that damage index is greater than zero
1-pt((0-10.82052)/(2.102*sqrt(1+1/23+((31-mean(T))^2/22/var(T)))),21)
```


```{r, eval=F, echo=T}
# http://www.randomservices.org/random/data/Galton.txt
# Galton's seminal data on predicting the height of children from the 
# heights of the parents, all in inches

heights=read.table("http://www.randomservices.org/random/data/Galton.txt",header=T)
attach(heights)
names(heights)

pairs(heights)
summary(lm(Height~Father+Mother+Gender+Kids))
summary(lm(Height~Father+Mother+Gender))
heights.lm=lm(Height~Father+Mother+Gender)

# each extra inch taller a father is is correlated with 0.4 inch extra height in the child
# each extra inch taller a mother is is correlated with 0.3 inch extra height in the child
# a male child is on average 5.2 inches taller than a female child
# 95% posterior interval for the the difference in height by gender
5.226 - 0.144*qt(.975,894)
5.226 + 0.144*qt(.975,894)

# posterior prediction interval (same as frequentist)
predict(heights.lm,data.frame(Father=68,Mother=64,Gender="M"),interval="predict")
predict(heights.lm,data.frame(Father=68,Mother=64,Gender="F"),interval="predict")

```


